{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Train U-Net on the CAS Landslide Detection Dataset\n",
    "\n",
    "*Authors: Abdelouahed Drissi*"
   ],
   "id": "b24639b8faae326b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataset",
   "id": "bd14e6d4dca9b41c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T13:39:01.100772Z",
     "start_time": "2024-11-10T13:38:47.998663Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Dropout, Lambda\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from losses import dice_loss"
   ],
   "id": "f2aa39bf640a894a",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Function to read the image file",
   "id": "ef25cc1ce83653cc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T13:40:11.137802Z",
     "start_time": "2024-11-10T13:40:11.130771Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_image_file(image_path, mask_path):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    mask = tf.io.read_file(mask_path)\n",
    "\n",
    "    image = tf.image.decode_png(image, channels=3)\n",
    "    mask = tf.image.decode_png(mask, channels=1)\n",
    "\n",
    "    return {\"image\": image, \"segmentation_mask\": mask}"
   ],
   "id": "7c131a4749b15ef0",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Loading the dataset",
   "id": "9bb40aa18495e789"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T13:41:10.236546Z",
     "start_time": "2024-11-10T13:40:13.941683Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Dataset paths\n",
    "train_image_dir = \"./dataset/train/images\"\n",
    "train_mask_dir = \"./dataset/train/masks\"\n",
    "valid_image_dir = \"./dataset/validation/images\"\n",
    "valid_mask_dir = \"./dataset/validation/masks\"\n",
    "test_image_dir = \"./dataset/test/images\"\n",
    "test_mask_dir = \"./dataset/test/masks\"\n",
    "\n",
    "# Load datasets and match images with masks\n",
    "def load_data(image_dir, mask_dir):\n",
    "    image_names = sorted(os.listdir(image_dir))\n",
    "    mask_names = sorted(os.listdir(mask_dir))\n",
    "    pairs = []\n",
    "    for img_name in image_names:\n",
    "        mask_name = img_name.replace(\"image\", \"mask\")\n",
    "        if mask_name in mask_names:\n",
    "            pairs.append((os.path.join(image_dir, img_name), os.path.join(mask_dir, mask_name)))\n",
    "    data = [load_image_file(image_path, mask_path) for image_path, mask_path in pairs]\n",
    "    return data\n",
    "\n",
    "data_train = load_data(train_image_dir, train_mask_dir)\n",
    "data_valid = load_data(valid_image_dir, valid_mask_dir)\n",
    "data_test = load_data(test_image_dir, test_mask_dir)\n",
    "\n",
    "len(data_train), len(data_valid), len(data_test)"
   ],
   "id": "b28d7289a619ca09",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1385, 396, 199)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Normalization and Image Resizing",
   "id": "c8b308c9cfb32486"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T13:41:10.351787Z",
     "start_time": "2024-11-10T13:41:10.343080Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Normalize and preprocess images and masks\n",
    "image_size = 256\n",
    "mean = tf.constant([0.485, 0.456, 0.406])\n",
    "std = tf.constant([0.229, 0.224, 0.225])\n",
    "\n",
    "\n",
    "def normalize(input_image, input_mask):\n",
    "    input_image = tf.image.convert_image_dtype(input_image, tf.float32)\n",
    "    input_image = (input_image - mean) / tf.maximum(std, backend.epsilon())\n",
    "    input_mask = input_mask / 255\n",
    "    return input_image, input_mask\n",
    "\n",
    "\n",
    "def load_image(datapoint):\n",
    "    input_image = tf.image.resize(datapoint[\"image\"], (image_size, image_size))\n",
    "    input_mask = tf.image.resize(\n",
    "        datapoint[\"segmentation_mask\"],\n",
    "        (image_size, image_size),\n",
    "        method=\"bilinear\",\n",
    "    )\n",
    "    \n",
    "    input_image, input_mask = normalize(input_image, input_mask)\n",
    "    return ({\"pixel_values\": input_image}, input_mask)"
   ],
   "id": "b53a964d770faf2",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T13:41:17.760950Z",
     "start_time": "2024-11-10T13:41:10.423301Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_data = [load_image(datapoint) for datapoint in data_train]\n",
    "valid_data = [load_image(datapoint) for datapoint in data_valid]\n",
    "test_data = [load_image(datapoint) for datapoint in data_test]"
   ],
   "id": "5b82256e375ac5e9",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Build input pipeline",
   "id": "def54426f243329b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T13:41:22.468722Z",
     "start_time": "2024-11-10T13:41:22.235260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 4\n",
    "auto = tf.data.AUTOTUNE\n",
    "# Create dataset generators\n",
    "def generator(data):\n",
    "    for datapoint in data:\n",
    "        yield datapoint\n",
    "\n",
    "\n",
    "train_ds = tf.data.Dataset.from_generator(\n",
    "    lambda: generator(train_data),\n",
    "    output_types=({\"pixel_values\": tf.float32}, tf.int32),\n",
    "    output_shapes=({\"pixel_values\": (image_size, image_size, 3)}, (image_size, image_size,1))\n",
    ").cache().shuffle(batch_size * 10).batch(batch_size).repeat().prefetch(auto)\n",
    "\n",
    "valid_ds = tf.data.Dataset.from_generator(\n",
    "    lambda: generator(valid_data),\n",
    "    output_types=({\"pixel_values\": tf.float32}, tf.int32),\n",
    "    output_shapes=({\"pixel_values\": (image_size, image_size, 3)}, (image_size, image_size,1))\n",
    ").batch(batch_size).repeat().prefetch(auto)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_generator(\n",
    "    lambda: generator(test_data),\n",
    "    output_types=({\"pixel_values\": tf.float32}, tf.int32),\n",
    "    output_shapes=({\"pixel_values\": (image_size, image_size, 3)}, (image_size, image_size,1))\n",
    ").batch(batch_size).prefetch(auto)\n"
   ],
   "id": "7117751c7736ec15",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T13:41:26.449087Z",
     "start_time": "2024-11-10T13:41:26.442085Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'pixel_values': TensorSpec(shape=(None, 256, 256, 3), dtype=tf.float32, name=None)}, TensorSpec(shape=(None, 256, 256, 1), dtype=tf.int32, name=None))\n"
     ]
    }
   ],
   "execution_count": 8,
   "source": "print(train_ds.element_spec)\n",
   "id": "6e8bec264e8cd9ec"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
